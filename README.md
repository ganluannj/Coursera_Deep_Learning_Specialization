# Deep_Learning_Practice
Programming assignment for Deep Learning Specialization on Coursera
* Initialization, Regularization, and Gradient Checking
  * Initialization: 
      * Different initialization lead to different result
      * Random initialization is used to break symmetry and make sure different hidden units can learn different things
      * Don't initialize to values that are too large
      * He initialization works well for networks with ReLU activation
